{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-11T13:32:38.348252Z","iopub.execute_input":"2023-02-11T13:32:38.348727Z","iopub.status.idle":"2023-02-11T13:32:38.376491Z","shell.execute_reply.started":"2023-02-11T13:32:38.348638Z","shell.execute_reply":"2023-02-11T13:32:38.375434Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"male = pd.read_csv(\"/kaggle/input/audiogen/line_index_male.tsv\", sep='\\t', names=['tag', 'content'])\nfemale = pd.read_csv(\"/kaggle/input/audiogen/line_index_female.tsv\", sep='\\t', names=['tag', 'content'])","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:32:38.378839Z","iopub.execute_input":"2023-02-11T13:32:38.379397Z","iopub.status.idle":"2023-02-11T13:32:38.438372Z","shell.execute_reply.started":"2023-02-11T13:32:38.379353Z","shell.execute_reply":"2023-02-11T13:32:38.437415Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/mostafaelaraby/wavegan-pytorch.git","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:32:38.439796Z","iopub.execute_input":"2023-02-11T13:32:38.440182Z","iopub.status.idle":"2023-02-11T13:32:40.143795Z","shell.execute_reply.started":"2023-02-11T13:32:38.440146Z","shell.execute_reply":"2023-02-11T13:32:40.142377Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'wavegan-pytorch'...\nremote: Enumerating objects: 78, done.\u001b[K\nremote: Counting objects: 100% (10/10), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 78 (delta 2), reused 6 (delta 2), pack-reused 68\u001b[K\nUnpacking objects: 100% (78/78), 2.14 MiB | 10.03 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport os\nos.chdir('wavegan-pytorch')","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:32:40.147061Z","iopub.execute_input":"2023-02-11T13:32:40.147430Z","iopub.status.idle":"2023-02-11T13:32:40.153463Z","shell.execute_reply.started":"2023-02-11T13:32:40.147395Z","shell.execute_reply":"2023-02-11T13:32:40.152185Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip3  install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:32:40.155681Z","iopub.execute_input":"2023-02-11T13:32:40.156677Z","iopub.status.idle":"2023-02-11T13:32:54.140790Z","shell.execute_reply.started":"2023-02-11T13:32:40.156555Z","shell.execute_reply":"2023-02-11T13:32:54.139443Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib>=2.2.4 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (3.5.2)\nRequirement already satisfied: numpy>=1.16.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.21.6)\nRequirement already satisfied: librosa>=0.6.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.9.2)\nCollecting pescador>=2.0.1\n  Downloading pescador-2.1.0.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.11.0)\nRequirement already satisfied: tqdm>=4.32.1 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (4.64.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.4->-r requirements.txt (line 1)) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.4->-r requirements.txt (line 1)) (1.4.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.4->-r requirements.txt (line 1)) (23.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.4->-r requirements.txt (line 1)) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.4->-r requirements.txt (line 1)) (2.8.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.4->-r requirements.txt (line 1)) (9.1.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.4->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (0.4.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (1.0.1)\nRequirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (0.11.0)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (1.6.0)\nRequirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (5.1.1)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (3.0.0)\nRequirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (0.55.2)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (1.0.2)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from librosa>=0.6.3->-r requirements.txt (line 3)) (1.7.3)\nRequirement already satisfied: six>=1.8 in /opt/conda/lib/python3.7/site-packages (from pescador>=2.0.1->-r requirements.txt (line 4)) (1.15.0)\nRequirement already satisfied: pyzmq>=15.0 in /opt/conda/lib/python3.7/site-packages (from pescador>=2.0.1->-r requirements.txt (line 4)) (23.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (4.1.1)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba>=0.45.1->librosa>=0.6.3->-r requirements.txt (line 3)) (0.38.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.45.1->librosa>=0.6.3->-r requirements.txt (line 3)) (59.8.0)\nRequirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from pooch>=1.0->librosa>=0.6.3->-r requirements.txt (line 3)) (1.4.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from pooch>=1.0->librosa>=0.6.3->-r requirements.txt (line 3)) (2.28.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->librosa>=0.6.3->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa>=0.6.3->-r requirements.txt (line 3)) (1.15.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.3->-r requirements.txt (line 3)) (2.21)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.3->-r requirements.txt (line 3)) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.3->-r requirements.txt (line 3)) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.3->-r requirements.txt (line 3)) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.3->-r requirements.txt (line 3)) (1.26.14)\nBuilding wheels for collected packages: pescador\n  Building wheel for pescador (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pescador: filename=pescador-2.1.0-py3-none-any.whl size=21105 sha256=d87f2b49fa6b07e21c7a61c2227312007abf3163aa9d7c5a872b152c6722f1e6\n  Stored in directory: /root/.cache/pip/wheels/f0/e3/c6/32d30d5eb5292dac352d2fca4ebf393aa94e09b9b8b4b0f341\nSuccessfully built pescador\nInstalling collected packages: pescador\nSuccessfully installed pescador-2.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile params.py\nimport torch\nimport random\nimport numpy as np\nimport logging\nimport os\n\n#############################\n# DataSet Path\n#############################s\ntarget_signals_dir = '/kaggle/input/audiogen/ta_in_male'\n#############################\n# Model Params\n#############################\nmodel_prefix = 'male' # name of the model to be saved\nn_iterations = 100000\nlr_g = 1e-4\nlr_d = 3e-4 # you can use with discriminator having a larger learning rate than generator instead of using n_critic updates ttur https://arxiv.org/abs/1706.08500\nbeta1 = 0.5\nbeta2 = 0.9\nuse_batchnorm=False\nvalidate=False\ndecay_lr = False # used to linearly deay learning rate untill reaching 0 at iteration 100,000\ngenerator_batch_size_factor = 1 # in some cases we might try to update the generator with double batch size used in the discriminator https://arxiv.org/abs/1706.08500\nn_critic = 1 # update generator every n_critic steps if lr_g = lr_d the n_critic's default value is 5 \n# gradient penalty regularization factor.\np_coeff = 10\nbatch_size = 10\nnoise_latent_dim = 100  # size of the sampling noise\nmodel_capacity_size = 32    # model capacity during training can be reduced to 32 for larger window length of 2 seconds and 4 seconds\n# rate of storing validation and costs params\nstore_cost_every = 300\nprogress_bar_step_iter_size = 400\n#############################\n# Backup Params\n#############################\ntake_backup = True\nbackup_every_n_iters = 1000\nsave_samples_every = 1000\noutput_dir = 'output'\nif not(os.path.isdir(output_dir)):\n    os.makedirs(output_dir)\n\nwindow_length = 65536 #[16384, 32768, 65536] in case of a longer window change model_capacity_size to 32\nsampling_rate = 16000\nnormalize_audio = True \nnum_channels = 1\n\n#############################\n# Logger init\n#############################\nLOGGER = logging.getLogger('wavegan')\nLOGGER.setLevel(logging.DEBUG)\n#############################\n# Torch Init and seed setting\n#############################\ncuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n# update the seed\nmanual_seed = 2019 \nrandom.seed(manual_seed)\ntorch.manual_seed(manual_seed)\nnp.random.seed(manual_seed)\nif cuda:\n    torch.cuda.manual_seed(manual_seed)\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:32:54.142983Z","iopub.execute_input":"2023-02-11T13:32:54.143381Z","iopub.status.idle":"2023-02-11T13:32:54.153443Z","shell.execute_reply.started":"2023-02-11T13:32:54.143337Z","shell.execute_reply":"2023-02-11T13:32:54.152251Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting params.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile utils.py\nimport os\nimport time\nimport math\nimport torch\nimport random\nimport librosa\nimport librosa.display\nimport numpy as np\nfrom torch.utils import data\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport glob\nimport pescador\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom params import *\nimport soundfile as sf\n#############################\n# File Utils\n#############################\ndef get_recursive_files(folderPath, ext):\n    results = os.listdir(folderPath)\n    outFiles = []\n    for file in results:\n        if os.path.isdir(os.path.join(folderPath, file)):\n            outFiles += get_recursive_files(os.path.join(folderPath, file), ext)\n        elif file.endswith(ext):\n            outFiles.append(os.path.join(folderPath, file))\n\n    return outFiles\n\n\ndef make_path(output_path):\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    return output_path\n\n\n#############################\n# Plotting utils\n#############################\ndef visualize_audio(audio_tensor, is_monphonic=False):\n    # takes a batch ,n channels , window length and plots the spectogram\n    input_audios = audio_tensor.detach().cpu().numpy()\n    plt.figure(figsize=(18, 50))\n    for i, audio in enumerate(input_audios):\n        plt.subplot(10, 2, i + 1)\n        if is_monphonic:\n            plt.title(\"Monophonic %i\" % (i + 1))\n            librosa.display.waveshow(audio[0], sr=sampling_rate)\n        else:\n            D = librosa.amplitude_to_db(np.abs(librosa.stft(audio[0])), ref=np.max)\n            librosa.display.specshow(D, y_axis=\"linear\")\n            plt.colorbar(format=\"%+2.0f dB\")\n            plt.title(\"Linear-frequency power spectrogram %i\" % (i + 1))\n    if not (os.path.isdir(\"visualization\")):\n        os.makedirs(\"visualization\")\n    plt.savefig(\"visualization/interpolation.png\")\n\n\ndef visualize_loss(loss_1, loss_2, first_legend, second_legend, y_label):\n    plt.figure(figsize=(10, 5))\n    plt.title(\"{} and {} Loss During Training\".format(first_legend, second_legend))\n    plt.plot(loss_1, label=first_legend)\n    plt.plot(loss_2, label=second_legend)\n    plt.xlabel(\"iterations\")\n    plt.ylabel(y_label)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.legend()\n    plt.show()\n    if not (os.path.isdir(\"visualization\")):\n        os.makedirs(\"visualization\")\n    plt.savefig(\"visualization/loss.png\")\n\n\ndef latent_space_interpolation(model, n_samples=10):\n    z_test = sample_noise(2)\n    with torch.no_grad():\n        interpolates = []\n        for alpha in np.linspace(0, 1, n_samples):\n            interpolate_vec = alpha * z_test[0] + ((1 - alpha) * z_test[1])\n            interpolates.append(interpolate_vec)\n\n        interpolates = torch.stack(interpolates)\n        generated_audio = model(interpolates)\n    visualize_audio(generated_audio, True)\n\n\n#############################\n# Wav files utils\n#############################\n# Fast loading used with wav files only of 8 bits\ndef load_wav(wav_file_path):\n    try:\n        audio_data, _ = librosa.load(wav_file_path, sr=sampling_rate)\n\n        if normalize_audio:\n            # Clip magnitude\n            max_mag = np.max(np.abs(audio_data))\n            if max_mag > 1:\n                audio_data /= max_mag\n    except Exception as e:\n        LOGGER.error(\"Could not load {}: {}\".format(wav_file_path, str(e)))\n        raise e\n    audio_len = len(audio_data)\n    if audio_len < window_length:\n        pad_length = window_length - audio_len\n        left_pad = pad_length // 2\n        right_pad = pad_length - left_pad\n        audio_data = np.pad(audio_data, (left_pad, right_pad), mode=\"constant\")\n\n    return audio_data.astype(\"float32\")\n\n\ndef sample_audio(audio_data, start_idx=None, end_idx=None):\n    audio_len = len(audio_data)\n    if audio_len == window_length:\n        # If we only have a single 1*window_length audio, just yield.\n        sample = audio_data\n    else:\n        # Sample a random window from the audio\n        if start_idx is None or end_idx is None:\n            start_idx = np.random.randint(0, (audio_len - window_length) // 2)\n            end_idx = start_idx + window_length\n        sample = audio_data[start_idx:end_idx]\n    sample = sample.astype(\"float32\")\n    assert not np.any(np.isnan(sample))\n    return sample, start_idx, end_idx\n\n\ndef sample_buffer(buffer_data, start_idx=None, end_idx=None):\n    audio_len = len(buffer_data) // 4\n    if audio_len == window_length:\n        # If we only have a single 1*window_length audio, just yield.\n        sample = buffer_data\n    else:\n        # Sample a random window from the audio\n        if start_idx is None or end_idx is None:\n            start_idx = np.random.randint(0, (audio_len - window_length) // 2)\n            end_idx = start_idx + window_length\n        sample = buffer_data[start_idx * 4 : end_idx * 4]\n    return sample, start_idx, end_idx\n\n\ndef wav_generator(file_path):\n    audio_data = load_wav(file_path)\n    while True:\n        sample, _, _ = sample_audio(audio_data)\n        yield {\"single\": sample}\n\n\ndef create_stream_reader(single_signal_file_list):\n    data_streams = []\n    for audio_path in single_signal_file_list:\n        stream = pescador.Streamer(wav_generator, audio_path)\n        data_streams.append(stream)\n    mux = pescador.ShuffledMux(data_streams)\n    batch_gen = pescador.buffer_stream(mux, batch_size)\n    return batch_gen\n\n\ndef save_samples(epoch_samples, epoch):\n    \"\"\"\n    Save output samples.\n    \"\"\"\n    sample_dir = make_path(os.path.join(output_dir, str(epoch)))\n\n    for idx, sample in enumerate(epoch_samples):\n        output_path = os.path.join(sample_dir, \"{}.wav\".format(idx + 1))\n        sample = sample[0]\n#         librosa.output.write_wav(output_path, sample, sampling_rate)\n        \n        sf.write(output_path, sample, sampling_rate)\n\n#############################\n# Sampling from model\n#############################\ndef sample_noise(size):\n    z = torch.FloatTensor(size, noise_latent_dim).to(device)\n    z.data.normal_()  # generating latent space based on normal distribution\n    return z\n\n\n#############################\n# Model Utils\n#############################\n\n\ndef update_optimizer_lr(optimizer, lr, decay):\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr * decay\n\n\ndef gradients_status(model, flag):\n    for p in model.parameters():\n        p.requires_grad = flag\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv1d):\n        m.weight.data.normal_(0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.fill_(0)\n        m.bias.data.fill_(0)\n    elif isinstance(m, nn.Linear):\n        m.bias.data.fill_(0)\n\n\n#############################\n# Creating Data Loader and Sampler\n#############################\nclass WavDataLoader:\n    def __init__(self, folder_path, audio_extension=\"wav\"):\n        self.signal_paths = get_recursive_files(folder_path, audio_extension)\n        self.data_iter = None\n        self.initialize_iterator()\n\n    def initialize_iterator(self):\n        data_iter = create_stream_reader(self.signal_paths)\n        self.data_iter = iter(data_iter)\n\n    def __len__(self):\n        return len(self.signal_paths)\n\n    def numpy_to_tensor(self, numpy_array):\n        numpy_array = numpy_array[:, np.newaxis, :]\n        return torch.Tensor(numpy_array).to(device)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        x = next(self.data_iter)\n        return self.numpy_to_tensor(x[\"single\"])\n\n\nif __name__ == \"__main__\":\n    # For debugging purposes\n    import time \n    start = time.time()\n    print(time.time() - start)\n    train_loader = WavDataLoader('/kaggle/input/audiogen/ta_in_male', \"wav\")\n    start = time.time()\n    for i in range(7):\n        x = next(train_loader)\n    print(time.time() - start)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:32:54.155823Z","iopub.execute_input":"2023-02-11T13:32:54.156557Z","iopub.status.idle":"2023-02-11T13:32:54.170709Z","shell.execute_reply.started":"2023-02-11T13:32:54.156486Z","shell.execute_reply":"2023-02-11T13:32:54.169259Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Overwriting utils.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile train.py\nfrom params import *\nfrom utils import *\nfrom models import *\nimport torch.optim as optim\nimport torch\nfrom torch.autograd import grad, Variable\nfrom tqdm import tqdm\n\n\nclass WaveGan_GP(object):\n    def __init__(self, train_loader, val_loader):\n        super(WaveGan_GP, self).__init__()\n        self.g_cost = []\n        self.train_d_cost = []\n        self.train_w_distance = []\n        self.valid_g_cost = [-1]\n        self.valid_reconstruction = []\n\n        self.discriminator = WaveGANDiscriminator(\n            slice_len=window_length,\n            model_size=model_capacity_size,\n            use_batch_norm=use_batchnorm,\n            num_channels=num_channels,\n        ).to(device)\n        self.discriminator.apply(weights_init)\n\n        self.generator = WaveGANGenerator(\n            slice_len=window_length,\n            model_size=model_capacity_size,\n            use_batch_norm=use_batchnorm,\n            num_channels=num_channels,\n        ).to(device)\n        self.generator.apply(weights_init)\n\n        self.optimizer_g = optim.Adam(\n            self.generator.parameters(), lr=lr_g, betas=(beta1, beta2)\n        )  # Setup Adam optimizers for both G and D\n        self.optimizer_d = optim.Adam(\n            self.discriminator.parameters(), lr=lr_d, betas=(beta1, beta2)\n        )\n\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.validate = validate\n        self.n_samples_per_batch = len(train_loader)\n\n    def calculate_discriminator_loss(self, real, generated):\n        disc_out_gen = self.discriminator(generated)\n        disc_out_real = self.discriminator(real)\n\n        alpha = torch.FloatTensor(batch_size, 1, 1).uniform_(0, 1).to(device)\n        alpha = alpha.expand(batch_size, real.size(1), real.size(2))\n\n        interpolated = (1 - alpha) * real.data + (alpha) * generated.data[:batch_size]\n        interpolated = Variable(interpolated, requires_grad=True)\n\n        # calculate probability of interpolated examples\n        prob_interpolated = self.discriminator(interpolated)\n        grad_inputs = interpolated\n        ones = torch.ones(prob_interpolated.size()).to(device)\n        gradients = grad(\n            outputs=prob_interpolated,\n            inputs=grad_inputs,\n            grad_outputs=ones,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True,\n        )[0]\n        # calculate gradient penalty\n        grad_penalty = (\n            p_coeff\n            * ((gradients.view(gradients.size(0), -1).norm(2, dim=1) - 1) ** 2).mean()\n        )\n        assert not (torch.isnan(grad_penalty))\n        assert not (torch.isnan(disc_out_gen.mean()))\n        assert not (torch.isnan(disc_out_real.mean()))\n        cost_wd = disc_out_gen.mean() - disc_out_real.mean()\n        cost = cost_wd + grad_penalty\n        return cost, cost_wd\n\n    def apply_zero_grad(self):\n        self.generator.zero_grad()\n        self.optimizer_g.zero_grad()\n\n        self.discriminator.zero_grad()\n        self.optimizer_d.zero_grad()\n\n    def enable_disc_disable_gen(self):\n        gradients_status(self.discriminator, True)\n        gradients_status(self.generator, False)\n\n    def enable_gen_disable_disc(self):\n        gradients_status(self.discriminator, False)\n        gradients_status(self.generator, True)\n\n    def disable_all(self):\n        gradients_status(self.discriminator, False)\n        gradients_status(self.generator, False)\n\n    def train(self):\n        progress_bar = tqdm(total=n_iterations // progress_bar_step_iter_size)\n        fixed_noise = sample_noise(batch_size).to(\n            device\n        )  # used to save samples every few epochs\n\n        gan_model_name = \"gan_{}.tar\".format(model_prefix)\n\n        first_iter = 0\n        if take_backup and os.path.isfile(gan_model_name):\n            if cuda:\n                checkpoint = torch.load(gan_model_name)\n            else:\n                checkpoint = torch.load(gan_model_name, map_location=\"cpu\")\n            self.generator.load_state_dict(checkpoint[\"generator\"])\n            self.discriminator.load_state_dict(checkpoint[\"discriminator\"])\n            self.optimizer_d.load_state_dict(checkpoint[\"optimizer_d\"])\n            self.optimizer_g.load_state_dict(checkpoint[\"optimizer_g\"])\n            self.train_d_cost = checkpoint[\"train_d_cost\"]\n            self.train_w_distance = checkpoint[\"train_w_distance\"]\n            self.valid_g_cost = checkpoint[\"valid_g_cost\"]\n            self.g_cost = checkpoint[\"g_cost\"]\n\n            first_iter = checkpoint[\"n_iterations\"]\n            for i in range(0, first_iter, progress_bar_step_iter_size):\n                progress_bar.update()\n            self.generator.eval()\n            with torch.no_grad():\n                fake = self.generator(fixed_noise).detach().cpu().numpy()\n            save_samples(fake, first_iter)\n        self.generator.train()\n        self.discriminator.train()\n        for iter_indx in range(first_iter, n_iterations):\n            self.enable_disc_disable_gen()\n            for _ in range(n_critic):\n                real_signal = next(self.train_loader)\n\n                # need to add mixed signal and flag\n                noise = sample_noise(batch_size * generator_batch_size_factor)\n                generated = self.generator(noise)\n                #############################\n                # Calculating discriminator loss and updating discriminator\n                #############################\n                self.apply_zero_grad()\n                disc_cost, disc_wd = self.calculate_discriminator_loss(\n                    real_signal.data, generated.data\n                )\n                assert not (torch.isnan(disc_cost))\n                disc_cost.backward()\n                self.optimizer_d.step()\n\n            if self.validate and iter_indx % store_cost_every == 0:\n                self.disable_all()\n                val_data = next(self.val_loader)\n                val_real = val_data\n                with torch.no_grad():\n                    val_discriminator_output = self.discriminator(val_real)\n                    val_generator_cost = val_discriminator_output.mean()\n                    self.valid_g_cost.append(val_generator_cost.item())\n\n            #############################\n            # (2) Update G network every n_critic steps\n            #############################\n            self.apply_zero_grad()\n            self.enable_gen_disable_disc()\n            noise = sample_noise(batch_size * generator_batch_size_factor)\n            generated = self.generator(noise)\n            discriminator_output_fake = self.discriminator(generated)\n            generator_cost = -discriminator_output_fake.mean()\n            generator_cost.backward()\n            self.optimizer_g.step()\n            self.disable_all()\n\n            if iter_indx % store_cost_every == 0:\n                self.g_cost.append(generator_cost.item() * -1)\n                self.train_d_cost.append(disc_cost.item())\n                self.train_w_distance.append(disc_wd.item() * -1)\n\n                progress_updates = {\n                    \"Loss_D WD\": str(self.train_w_distance[-1]),\n                    \"Loss_G\": str(self.g_cost[-1]),\n                    \"Val_G\": str(self.valid_g_cost[-1]),\n                }\n                progress_bar.set_postfix(progress_updates)\n\n            if iter_indx % progress_bar_step_iter_size == 0:\n                progress_bar.update()\n            # lr decay\n            if decay_lr:\n                decay = max(0.0, 1.0 - (iter_indx * 1.0 / n_iterations))\n                # update the learning rate\n                update_optimizer_lr(self.optimizer_d, lr_d, decay)\n                update_optimizer_lr(self.optimizer_g, lr_g, decay)\n\n            if iter_indx % save_samples_every == 0:\n                with torch.no_grad():\n                    latent_space_interpolation(self.generator, n_samples=2)\n                    fake = self.generator(fixed_noise).detach().cpu().numpy()\n                save_samples(fake, iter_indx)\n\n            if take_backup and iter_indx % backup_every_n_iters == 0:\n                saving_dict = {\n                    \"generator\": self.generator.state_dict(),\n                    \"discriminator\": self.discriminator.state_dict(),\n                    \"n_iterations\": iter_indx,\n                    \"optimizer_d\": self.optimizer_d.state_dict(),\n                    \"optimizer_g\": self.optimizer_g.state_dict(),\n                    \"train_d_cost\": self.train_d_cost,\n                    \"train_w_distance\": self.train_w_distance,\n                    \"valid_g_cost\": self.valid_g_cost,\n                    \"g_cost\": self.g_cost,\n                }\n                torch.save(saving_dict, gan_model_name)\n\n        self.generator.eval()\n\n\nif __name__ == \"__main__\":\n    train_loader = WavDataLoader(\"/kaggle/input/audiogen/ta_in_male\")\n    val_loader = WavDataLoader(\"/kaggle/input/audiogen/ta_in_female\")\n\n    wave_gan = WaveGan_GP(train_loader, val_loader)\n    wave_gan.train()\n    visualize_loss(\n        wave_gan.g_cost, wave_gan.valid_g_cost, \"Train\", \"Val\", \"Negative Critic Loss\"\n    )\n    latent_space_interpolation(wave_gan.generator, n_samples=5)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:32:54.172903Z","iopub.execute_input":"2023-02-11T13:32:54.173294Z","iopub.status.idle":"2023-02-11T13:32:54.189216Z","shell.execute_reply.started":"2023-02-11T13:32:54.173256Z","shell.execute_reply":"2023-02-11T13:32:54.188199Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}]},{"cell_type":"code","source":"\n!python3  train.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}